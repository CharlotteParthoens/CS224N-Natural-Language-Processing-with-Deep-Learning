\section{Related Work}
\label{sec:relatedwork}

Machine comprehension (MC) and question answering (QA) tasks have gained significant interest in the past few years. In this section we provide a brief overview of some of the fundamental techniques in this field.

\paragraph{Use of Attention mechanisms.} The use of attention mechanisms is one of the key tenets of existing MC literature. Several techniques \cite{bahdanau2014neural, hermann2015teaching} use a dynamic attention mechanism, in which the attention weights are updated dynamically using the query, context and previous attention. Chen at al \cite{chen2016thorough} show that simply using a bi-linear term for computing the attention weights improves model accuracy. In this paper, we use a memory-less attention mechanism similar to BiDAF \cite{seo2016bidirectional}. We introduce the notion of early attention, and show how it helps model performance.

\paragraph{RNN, CNN and Transformer architectures.} Recurrent Neural Networks have traditionally been the model of choice to capture the sequential nature of text. While common, RNNs are slow because of their recursive definition, which prevents parallel computation. Another drawback of RNNs is difficulty in modeling long dependencies, an issue which has prompted the use of Gated Recurrent Unit (GRU) and Long Short Term Memory (LSTM) architectures for NLP tasks. Attempts have also been made to replace recurrent networks by Convolution Neural Networks (CNN) \cite{kim2014convolutional}. More recently, full attention architectures \cite{vaswani2017attention} have been shown to be effective across a wide variety of tasks, such as text classification, machine translation, question answering and sentiment analysis. A combination of convolutions and self-attention \cite{yu2018qanet} has also been shown to have promising results. In this paper, we use the best of all worlds using a combination of RNN, CNN and attention mechanisms in a hierarchical model.

\paragraph{Pre-trained Contextual Embeddings.} Several state of the art techniques leverage pre-trained contextual embeddings (PCE). Examples of such PCE-based techniques are ELMo \cite{peters2018deep} and BERT \cite{devlin2018bert}. The core idea of such techniques is to represent a piece of text using word embeddings that depend on the context in which the word appears in the text. This is typically achieved by pretraining the weights on a large-scale language modeling dataset, and using the pre-trained weights for the initial model layers.  In this paper we do not use PCE-based techniques. As mentioned in Section \ref{subsec:models} we use GloVe embeddings to represent our word vectors. We can extend our model to incorporate PCE-based techniques.  
