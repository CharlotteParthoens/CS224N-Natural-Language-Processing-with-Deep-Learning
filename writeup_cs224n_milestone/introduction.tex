\section{Introduction}
\label{sec:introduction}

Machine comprehension (MC) and question answering (QA) tasks have gained significant interest in the past few years, with several end-to-end models showing promising results. A key factor in recent advancements has been the use of neural attention mechanisms, which extract useful signal by exploiting the notion of \textit{matching}. 

Several attention approaches have been proposed in literature. Chen et al \cite{chen2016thorough} propose a uni-directional attention mechanism whereby the query attends to the context paragraph. In BiDAF, Seo at al \cite{seo2016bidirectional} introduce bi-directional attention flow to obtain query-aware context representations. Wang et al \cite{wang2017gated} note that question-aware passage representations have limited knowledge of the context itself, and motivate self-matching attention to directly match the question-aware passage representation against itself.
 
Another key tenet of the proposed techniques is to use a model to process sequential inputs. This is typically done in the form of an embedding encoder layer. While recurrent neural networks have been the model of choice for this, recent work by Yu et al \cite{yu2018qanet} propose using a combination of convolution and self-attention mechanisms instead. 

In this paper we explore two novel extensions. First, we observe that given the input embeddings, most existing models dive straight into the encoding layer. Attention is an afterthought, that gets applied only in later layers of the model. A problem with such representations is that it strains the embedding encoder layer, with the rest of the modeling layers all stacked on top. We propose adding a `Embedding Attention Layer' as a form of self-attention over the word and character input embeddings. Second, we propose having a contextual embed layer that uses a combination of recurrent layers and convolution layers, with the motivation of bringing the best of both worlds in the contextual embedding space. We show that adding these two extensions is indeed helpful, and show our comparisons with respect to the BiDAF model \cite{seo2016bidirectional}

The remainder of the paper is organized as follows. In Section ~\ref{sec:model} we introduce our model. Section ~\ref{sec:experiment} presents the experimental results from our modeling techniques. In Section \ref{sec:relatedwork} we survey some related work in the area of machine comprehension. Finally, we present our conclusions in Section ~\ref{sec:conclusion}

 






