\section{Introduction}
\label{sec:introduction}

Machine comprehension (MC) and question answering (QA) tasks have gained significant interest in the past few years, with several end-to-end models showing promising results. 

A key tenet of existing techniques is to use an embedding encoder layer. The input embedding layer first maps each word to a vector space. The embedding encoder layer then refines the embeddings using contextual cues from surrounding words. Approaches such as BiDAF \cite{seo2016bidirectional} use a Long Short-Term Memory (LSTM) network at the embedding encoder layer to model temporal interactions between words, while more recent approaches such as QANet \cite{yu2018qanet} use a combination of convolution and self-attention to model this layer. The embedding encoder layer plays a crucial in the model hierarchy with other layers stacked in a hierarchical fashion on top of it. 

Another key factor in recent advancements has been the use of neural attention mechanisms, which extract useful signal by exploiting the notion of matching; either matching \{context, query\} sentences in QA tasks, or matching \{source, target\} language sentences in machine translation tasks. Several attention approaches have been proposed in literature. Chen et al \cite{chen2016thorough} propose a uni-directional attention mechanism whereby the query attends to the context paragraph. In BiDAF, Seo at al \cite{seo2016bidirectional} introduce bi-directional attention flow to obtain query-aware context representations. Wang et al \cite{wang2017gated} note that query-aware passage representations have limited knowledge of the context itself, and motivate self-matching attention to directly match the query-aware passage representation against itself.

In this paper we propose two novel extensions. First, we observe that given the input embeddings, most existing models dive straight into the embedding encoding layer. Attention layers come in later in the modeling stack, almost as an afterthought.  We propose adding a embedding attention layer, as a form of early attention over the word and char embeddings. Second, we propose having a combination of recurrent and convolution layers in the embedding encoder layer. The motivation for these two novelties is to bring the benefits of \textit{early attention} and \textit{local convolution} into the model hierarchy. We show that adding these two extensions is indeed helpful, and helps outperform the BiDAF \cite{seo2016bidirectional} model proposed recently in literature.  

The remainder of the paper is organized as follows. In Section \ref{sec:model} we introduce our model. Section \ref{sec:experiment} presents the experimental results from our modeling techniques. In Section \ref{sec:relatedwork} we survey related work in the field of machine comprehension. Finally, we present our conclusions in Section \ref{sec:conclusion}

 






