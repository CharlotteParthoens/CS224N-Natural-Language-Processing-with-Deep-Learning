\documentclass[]{article}

%%%%%%%%%%%%%%%%%%%
% Packages/Macros %
%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb,latexsym,amsmath}     % Standard packages
\usepackage{graphicx}

\usepackage[english]{babel}
\usepackage{csquotes}
		
%%%%%%%%%%%
% Margins %
%%%%%%%%%%%
\addtolength{\textwidth}{1.0in}
\addtolength{\textheight}{1.00in}
\addtolength{\evensidemargin}{-0.75in}
\addtolength{\oddsidemargin}{-0.75in}
\addtolength{\topmargin}{-.50in}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Theorem/Proof Environments %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{theorem}{Theorem}
\newenvironment{proof}{\noindent{\bf Proof:}}{$\hfill \Box$ \vspace{10pt}}  


%%%%%%%%%%%%
% Document %
%%%%%%%%%%%%
\begin{document}

\title{CS 224n Assignment 3.}
\author{Abhishek Goswami.}
\maketitle

\begin{enumerate}
	\item Machine Learning \& Neural Networks
	
	\begin{enumerate}
		
		% 1 (a) Adam Optimizer
		\item
		
		Adam Optimizer.
		
		\begin{enumerate}
				
			% 1 (a) (i) 
			\item
			
			$\boldsymbol{m}$ is rolling average of the gradients. $\beta_1$ is a hyper parameter between 0 and 1. 
			
		\begin{align}
		\boldsymbol{m} \leftarrow \beta_{1} \boldsymbol{m} + (1 - \beta_{1}) \nabla_{\boldsymbol{\theta}} \boldsymbol{J}_\textsubscript{minibatch}(\boldsymbol{\theta}).
		\end{align}
		
			The fact that $\beta_1$ is set to a high value (0.9 often) suggests that $\boldsymbol{m}$ is changing very little at each step.  This in turn ensures that the  model parameters  do not bounce around when moving towards the local optimum.
			% 1 (a) (ii) 
			\item
			$\boldsymbol{v}$ is the rolling average of the magnitude of the gradients. 
			
		\begin{align}
		\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \alpha \bigodot \boldsymbol{m} / \sqrt{\boldsymbol{v}}.
		\end{align}
			
			Since we are dividing by $\sqrt{\boldsymbol{v}}$ it means that the larger  gradients will get smalller updates.   Conversely,  the smaller gradients will get larger updates.  This will help the learning algorithm to move off flat areas.
			
		\end{enumerate}
		
		% 1 (b) Dropout		
		\item
		
		Dropout.

		\begin{enumerate}
		
			% 1 (b) (i) 
			\item		
			
				$\gamma$ in terms of $p_{drop}$
				
			\begin{align}
			\begin{split}
				h_i &= \mathbb{E}_{p_{drop}}[\mathbf{h}_{drop}]_{i} \\ 
							&= \mathbb{E}_{p_{drop}}[\gamma d_{i} h_{i}] \\ 
							&= p_{drop} (0) + (1 - p_{drop}) \gamma h_i \\ 
							&= (1 - p_{drop}) \gamma h_i
			\end{split}
			\end{align}
			
			So, 
			
			\begin{align}
			\gamma = \frac {1} {1 - p_{drop}} \\ 
			\end{align}
			
			% 1 (b) (ii) 
			\item
			
			Dropout is a regularization technique that we want to use during training to prevent overfitting on the training data. 

			At test time, we should not do dropout, else it would result in randomness in predictions. One thing we should do during test time is to scale the predictions appropriately  to account for the expectation term $\mathbb{E}_{p_{drop}}[\mathbf{h}_{drop}]_{i}$. Alternatively, we can use \textbf{inverted dropout} so we do the scaling at training time itself, and leave the code untouched during test time. 
												
    \end{enumerate}
  
	\end{enumerate}

	\item Neural Transition-Based Dependency Parsing
		
	\begin{enumerate}
	% 2 (a)
	\item Table \ref{table:sentencetransitions} shows the sequence of transitions needed for parsing the sentence : \enquote{\textit{I parsed this sentence correctly}}
		
\begin{table}[]
\begin{tabular}{|l|l|l|l|}
\hline
Stack                              & Buffer                                     & New dependency                 & Transition \\ \hline
{[}ROOT{]}                         & {[}I, parsed, this, sentence, correctly{]} &                                & start      \\ \hline
{[}ROOT, I{]}                      & {[}parsed, this, sentence, correctly{]}    &                                & SHIFT      \\ \hline
{[}ROOT, I, parsed{]}              & {[}this, sentence, correctly{]}            &                                & SHIFT      \\ \hline
{[}ROOT, parsed{]}                 & {[}this, sentence, correctly{]}            & parsed-\textgreater{}I         & LEFT-ARC   \\ \hline
{[}ROOT, parsed, this{]}           & {[}sentence, correctly{]}                  &                                & SHIFT      \\ \hline
{[}ROOT, parsed, this, sentence{]} & {[}correctly{]}                            &                                & SHIFT      \\ \hline
{[}ROOT, parsed, sentence{]}       & {[}correctly{]}                            & sentence-\textgreater{}this    & LEFT-ARC   \\ \hline
{[}ROOT, parsed{]}                 & {[}correctly{]}                            & parsed-\textgreater{}sentence  & RIGHT-ARC  \\ \hline
{[}ROOT, parsed, correctly{]}      & {[}{]}                                     &                                & SHIFT      \\ \hline
{[}ROOT, parsed{]}                 & {[}{]}                                     & parsed-\textgreater{}correctly & RIGHT-ARC  \\ \hline
{[}ROOT{]}                         & {[}{]}                                     & ROOT-\textgreater{}parsed      & RIGHT-ARC  \\ \hline
\end{tabular}
\caption{Transitions for sentence \enquote{\textit{I parsed this sentence correctly}}.}
\label{table:sentencetransitions}
\end{table}
	
	
	%2 (b)
	\item A sentence containing $n$ words will be parsed in $2n$ steps.  
	It is either (a) SHIFT  or (b)  one of $\{LEFT-ARC | RIGHT-ARC\} $
	% 2 (c)
	\item Coding exercise.
	% 2 (d)
	\item Coding exercise.
	% 2 (e)
	\item dev UAS: 88.36. test UAS: 88.90
	% 2 (f)
	\item Example dependency parses.
	
		\begin{enumerate}
				% 2 (f) i
				\item 
					\begin{description}
						\item[$\bullet$ Error type] item 1
						\item[$\bullet$ Incorrect dependency] item 2
						\item[$\bullet$ Correct dependency] item 3
					\end{description}
				% 2 (f) i
				\item 
					\begin{description}
						\item[$\bullet$ Error type] item 1
						\item[$\bullet$ Incorrect dependency] item 2
						\item[$\bullet$ Correct dependency] item 3
					\end{description}
				% 2 (f) i
				\item 
					\begin{description}
						\item[$\bullet$ Error type] item 1
						\item[$\bullet$ Incorrect dependency] item 2
						\item[$\bullet$ Correct dependency] item 3
					\end{description}
				% 2 (f) i
				\item 
					\begin{description}
						\item[$\bullet$ Error type] item 1
						\item[$\bullet$ Incorrect dependency] item 2
						\item[$\bullet$ Correct dependency] item 3
					\end{description}
			\end{enumerate}	
	\end{enumerate}	
\end{enumerate}

\end{document}